{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234eed3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: I:\\project2026\\llmagent\\RQs\\RQ3\\RQ3_corpus_level_gpt4o.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "IGNORE_FIELDS = {\"Raw_rows_first_100\", \"Exploration_sql\", \"Extraction_sql\", \"PII_Prompt\"}\n",
    "\n",
    "\n",
    "def _dedupe_preserve_order(items):\n",
    "    \"\"\"\n",
    "    Stable dedupe for lists that may contain scalars, dicts, or lists.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in items:\n",
    "        key = json.dumps(x, sort_keys=True, ensure_ascii=False) if isinstance(x, (dict, list)) else x\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(x)\n",
    "    return out\n",
    "\n",
    "\n",
    "def prefix_source_columns(db_path: str, cols: list) -> list:\n",
    "    \"\"\"\n",
    "    Prefix each source column with the database filename to avoid ambiguity\n",
    "    after aggregating across many DBs.\n",
    "\n",
    "    Example:\n",
    "      db_path = selectedDBs\\\\A1_msgstore.db\n",
    "      col     = message.text_data\n",
    "      -> A1_msgstore.db:message.text_data\n",
    "    \"\"\"\n",
    "    db_file = Path(db_path).name\n",
    "    out = []\n",
    "    for c in cols:\n",
    "        if isinstance(c, str) and c:\n",
    "            out.append(f\"{db_file}:{c}\")\n",
    "    return out\n",
    "\n",
    "\n",
    "def aggregate_jsonl_folder_corpus_level(in_dir: str | Path, out_path: str | Path) -> Path:\n",
    "    \"\"\"\n",
    "    Corpus-level aggregation across all *.jsonl files in in_dir, grouped ONLY by PII_type.\n",
    "\n",
    "    Input records are expected to already be normalized (your batch_results_normalized),\n",
    "    but this function still performs dedupe at aggregation time.\n",
    "\n",
    "    Output per PII_type keeps:\n",
    "      - PII_type\n",
    "      - PII_all: concatenated across corpus (with duplicates)\n",
    "      - PII_unique: deduped\n",
    "      - Num_of_PII_all: total count with duplicates (sum of per-record Num_of_PII or len(PII))\n",
    "      - Num_of_PII_unique: len(PII_unique)\n",
    "      - source_columns: deduped, prefixed with db filename\n",
    "      - Num_of_source_columns: len(source_columns)\n",
    "\n",
    "    It ignores IGNORE_FIELDS and discards all other keys.\n",
    "    \"\"\"\n",
    "    in_dir = Path(in_dir)\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    grouped: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    for jsonl_file in sorted(in_dir.glob(\"*.jsonl\")):\n",
    "        with jsonl_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            for line_no, line in enumerate(f, start=1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    rec = json.loads(line)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    raise ValueError(f\"Bad JSON in {jsonl_file} line {line_no}: {e}\") from e\n",
    "\n",
    "                pii_type = rec.get(\"PII_type\")\n",
    "                if not pii_type:\n",
    "                    continue\n",
    "\n",
    "                if pii_type not in grouped:\n",
    "                    grouped[pii_type] = {\n",
    "                        \"PII_type\": pii_type,\n",
    "                        \"PII_all\": [],\n",
    "                        \"PII_unique\": [],\n",
    "                        \"Num_of_PII_all\": 0,\n",
    "                        \"Num_of_PII_unique\": 0,\n",
    "                        \"source_columns\": [],\n",
    "                        \"Num_of_source_columns\": 0,\n",
    "                    }\n",
    "\n",
    "                agg = grouped[pii_type]\n",
    "\n",
    "                # --- PII + count (with-dup) ---\n",
    "                pii_list = rec.get(\"PII\", [])\n",
    "                if isinstance(pii_list, list):\n",
    "                    agg[\"PII_all\"].extend(pii_list)\n",
    "\n",
    "                n = rec.get(\"Num_of_PII\")\n",
    "                if isinstance(n, (int, float)) and not isinstance(n, bool):\n",
    "                    agg[\"Num_of_PII_all\"] += int(n)\n",
    "                else:\n",
    "                    agg[\"Num_of_PII_all\"] += len(pii_list) if isinstance(pii_list, list) else 0\n",
    "\n",
    "                # --- source_columns (with-dup) ---\n",
    "                dbp = rec.get(\"db_path\", \"\")\n",
    "                cols = rec.get(\"source_columns\", [])\n",
    "                if isinstance(cols, list):\n",
    "                    agg[\"source_columns\"].extend(prefix_source_columns(dbp, cols))\n",
    "\n",
    "                # ignore everything else (and IGNORE_FIELDS)\n",
    "\n",
    "    # --- Finalize: dedupe lists + compute unique counts ---\n",
    "    for agg in grouped.values():\n",
    "        agg[\"PII_unique\"] = _dedupe_preserve_order(agg[\"PII_all\"])\n",
    "        agg[\"Num_of_PII_unique\"] = len(agg[\"PII_unique\"])\n",
    "\n",
    "        agg[\"source_columns\"] = _dedupe_preserve_order(agg[\"source_columns\"])\n",
    "        agg[\"Num_of_source_columns\"] = len(agg[\"source_columns\"])\n",
    "\n",
    "    # --- Write aggregated JSONL ---\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for pii_type in sorted(grouped.keys()):\n",
    "            f.write(json.dumps(grouped[pii_type], ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    out = aggregate_jsonl_folder_corpus_level(\n",
    "        r\"..\\batch_results_gpt4o_normalized\",\n",
    "        \"RQ3_corpus_level_gpt4o.jsonl\",\n",
    "    )\n",
    "    print(f\"Wrote: {out.resolve()}\")\n",
    "    \n",
    "    \n",
    "    out = aggregate_jsonl_folder_corpus_level(\n",
    "        r\"..\\ground_truth_normalized\",\n",
    "        \"RQ3_corpus_level_ground_truth.jsonl\",\n",
    "    )\n",
    "    print(f\"Wrote: {out.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
